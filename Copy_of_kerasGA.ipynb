{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_kerasGA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lqbeWAze76Q",
        "scrolled": true
      },
      "source": [
        "#!pip install pygad"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu_yNfc2kEBP"
      },
      "source": [
        "Modified version of pygad samples. https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#life-cycle-of-pygad <br>\n",
        "Model inspired and modified from http://uhurumkate.blogspot.com/2018/06/tiny-model-for-mnist-dataset.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N1PbHsWQWlW"
      },
      "source": [
        "You're highly encouraged to use a GPU as it's going to spent most resources on predicting and calculating acc. Please also note that pygad library only use CPU paralleal in some cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXxmJd3ai2yd"
      },
      "source": [
        "The highest acc was achieved with everything set to False, and with 23 hours of training. 500 cycles and 58% acc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5L3JMNgS-L0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import losses, datasets, layers, optimizers, Sequential, metrics\n",
        "import pygad.kerasga\n",
        "import numpy\n",
        "import pygad"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdZ1owxPPsRn"
      },
      "source": [
        "# Hyper param"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGFxRvr3eWGX"
      },
      "source": [
        "acc_meter = tf.keras.metrics.Accuracy()\n",
        "\n",
        "#Instead of training a entire dataset, do small batches.\n",
        "sgd_like = True\n",
        "\n",
        "#introduce new population when stucked. Often time it'll stuck in local best solution.\n",
        "introduce_new_pop = True\n",
        "\n",
        "#increase batch size when stcuked.\n",
        "#Error occurs when using google colab. Not sure if it's google issue. \n",
        "#Will try on my desktop once I return next friday.\n",
        "dynamic_batch_size = True"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl1q_FZnfTG1"
      },
      "source": [
        "#num cyckes\n",
        "num_generations = 100\n",
        "\n",
        "#num soltions\n",
        "num_solution= 10\n",
        "\n",
        "#the following two param only works if introduce_new_pop = True\n",
        "#Maxium cycles stuck before introducing new population to the pool.\n",
        "max_cycles_stucked = 20\n",
        "#Number of population being replaced with new random population. Must be 0 < x <= num_solution\n",
        "num_introduce = 9\n",
        "\n",
        "#The follow two param have effect only when sgd_like = True\n",
        "#number of cycles you want on a single batch. \n",
        "epoch = 30\n",
        "#number of samples per batch\n",
        "batch_size = 300\n",
        "\n",
        "#number batch increase feed into prediction. only have effect if sgd_like = True\n",
        "#Ex. batch size 300 -> 300 + 300 = 600. Note that some data will be lose during the process\n",
        "batch_increase = 500\n",
        "\n",
        "#Typical pygad params\n",
        "num_parents_mating = int(num_solution * .7)\n",
        "parent_selection_type = \"sss\"\n",
        "mutation_type=\"adaptive\"\n",
        "mutation_num_genes=(90, 80)\n",
        "keep_parents = 3"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm_Hl7BAgrzv"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rkq5w4dequA"
      },
      "source": [
        "#load mnist dataset\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
        "#normalize data\n",
        "X_train = 2*tf.convert_to_tensor(X_train, tf.float32)/255.-1\n",
        "#[b, 28, 28] -> [b, 28, 28, 1]\n",
        "X_train = tf.reshape(X_train, (-1, 28,28,1))\n",
        "X_test = 2*tf.convert_to_tensor(X_test, tf.float32)/255.-1\n",
        "X_test = tf.reshape(X_test, (-1, 28,28,1))\n",
        "y_train = tf.convert_to_tensor(y_train, tf.int32)\n",
        "#[b] -> [b, 10]\n",
        "y_train = tf.one_hot(y_train, 20)\n",
        "y_test = tf.convert_to_tensor(y_test, tf.int32)\n",
        "y_test = tf.one_hot(y_test, 10)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGsLcsClp1P6"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYvgt7ULiR5L"
      },
      "source": [
        "We want a very small model for the increase in param will increase the cycles GA needs to run </ba>\n",
        "Model parameter = 2665 with acc ~ 96% on both train/testing sets (trained with triditional method).\n",
        "I relized that the model DO have impact. For example, when layers.Dense(3, activation='relu'), you'll have 1.8k param. You might think it'll be easier for the model to converge. Yet, layers.Dense(10, activation='relu') is faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW1AXe0zci0T",
        "outputId": "45e08820-7eb1-461a-f95b-14aecfa8b3ca"
      },
      "source": [
        "model = Sequential([\n",
        "#input layer\n",
        "layers.Convolution2D(15, (3, 3), activation='relu', input_shape=(28,28,1)),\n",
        "layers.BatchNormalization(momentum=0.1),\n",
        "layers.Dropout(0.1),\n",
        "layers.AveragePooling2D(2),\n",
        "#convo layers1\n",
        "layers.Convolution2D(10,(1,1), activation='relu'),\n",
        "layers.BatchNormalization(momentum=0.1),\n",
        "layers.Dropout(0.1),\n",
        "#convo layers2\n",
        "layers.AveragePooling2D(2),\n",
        "layers.Convolution2D(5,(3,3), activation='relu'),\n",
        "layers.BatchNormalization(momentum=0.1),\n",
        "layers.Dropout(0.1),\n",
        "#convo layers3\n",
        "layers.Convolution2D(10,(1,1), activation='relu'),\n",
        "layers.Dropout(0.1),\n",
        "#fully connected layer\n",
        "layers.Dense(10, activation='relu'),\n",
        "layers.Flatten(),\n",
        "#output layer\n",
        "layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 15)        150       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 26, 26, 15)        60        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 26, 26, 15)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 13, 13, 15)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 10)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 13, 13, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 10)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 5)           455       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 4, 5)           20        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 5)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 10)          60        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 10)          0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4, 4, 10)          110       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1610      \n",
            "=================================================================\n",
            "Total params: 2,665\n",
            "Trainable params: 2,605\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 15)        150       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 26, 26, 15)        60        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 26, 26, 15)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 13, 13, 15)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 13, 13, 10)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 13, 13, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 13, 13, 10)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 4, 4, 5)           455       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 4, 4, 5)           20        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 5)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 10)          60        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 4, 4, 10)          0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4, 4, 10)          110       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1610      \n",
            "=================================================================\n",
            "Total params: 2,665\n",
            "Trainable params: 2,605\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy2Zi5Wdip3p"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "#model.fit(X_train, y_train, \n",
        "#          batch_size=8, verbose=1,shuffle=True,\n",
        "#          validation_data=(X_test, y_test))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV6R1V52ptP7"
      },
      "source": [
        "Network accuracy and error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSVRcjVijtHY",
        "outputId": "33e20b4a-432c-4149-cb9a-5ac468f5604b"
      },
      "source": [
        "acc_meter.update_state(tf.argmax(model.predict(X_test), axis=1), tf.argmax(y_test, axis=1))\n",
        "print(\"Accuracy on testing set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
        "print(\"Error rate on testing set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))\n",
        "acc_meter.update_state(tf.argmax(model.predict(X_train), axis=1), tf.argmax(y_train, axis=1))\n",
        "print(\"Accuracy on training set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
        "print(\"Error rate on training set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set:15.26000052690506%\n",
            "Error rate on testing set:84.73999947309494%\n",
            "Accuracy on training set:14.815714955329895%\n",
            "Error rate on training set:85.1842850446701%\n",
            "Accuracy on testing set:22.750000655651093%\n",
            "Error rate on testing set:77.24999934434891%\n",
            "Accuracy on training set:21.962857246398926%\n",
            "Error rate on training set:78.03714275360107%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ptOJ9miprDS"
      },
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_omnjFA5lOh"
      },
      "source": [
        "#cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "#cce(y_train, model.predict(X_train))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1Q4nna4YeCz"
      },
      "source": [
        "#we don't need one-hot to train it's for tf.fit(). To compute tf.argmax each loop is a waste of resources\n",
        "y_train = tf.argmax(y_train, axis=1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYoIXfZjp4kJ"
      },
      "source": [
        "Perpare batches if we were doing SGD-like method. <br>\n",
        "Some data are removed if size of dataset / batch_size is not an integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMeNPbTi_E04"
      },
      "source": [
        "#A method always modifing X_train_sgd and y_train_sgd\n",
        "def make_batch(batch_size):\n",
        "  global X_train_sgd, y_train_sgd\n",
        "  if len(X_train) % batch_size == 0:\n",
        "    X_train_sgd = tf.reshape(X_train, [-1, batch_size, 28, 28, 1])\n",
        "    y_train_sgd = tf.reshape(y_train, [-1, batch_size])\n",
        "  else:\n",
        "    #Remove some samples if can't rehape \n",
        "    len_remove = len(X_train) % batch_size\n",
        "    X_train_sgd = tf.reshape(X_train[:-len_remove], [-1, batch_size, 28, 28, 1])\n",
        "    y_train_sgd = tf.reshape(y_train[:-len_remove], [-1, batch_size])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V7t28kieeVR"
      },
      "source": [
        "X_train_sgd = []\n",
        "y_train_sgd = []\n",
        "if sgd_like == True:\n",
        "  make_batch(batch_size)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROmBNKL2Pw2u"
      },
      "source": [
        "# Fitness "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuGqTK0NeeVR"
      },
      "source": [
        "#Some helper varibles\n",
        "i = 0\n",
        "cycles_stucked = 0\n",
        "k=0\n",
        "tmp = 0\n",
        "\n",
        "#fitness_func uses acc matrix as fitness.\n",
        "def fitness_func(solution, sol_idx):\n",
        "\n",
        "    global keras_ga, model, i, cycles_stucked ,X_train, y_train, k\n",
        "    solution_fitness = 0\n",
        "    model_weights_matrix = pygad.kerasga.model_weights_as_matrix(model=model,\n",
        "                                                                 weights_vector=solution)\n",
        "\n",
        "    model.set_weights(weights=model_weights_matrix)\n",
        "\n",
        "    #if perfer to use loss instead\n",
        "    #solution_fitness = 1/cce(model.predict(X_train), y_train).numpy()\n",
        "\n",
        "    if sgd_like == True:\n",
        "      #update acc_meter\n",
        "      acc_meter.update_state(tf.argmax(model.predict(X_train_sgd[k]), axis=1), y_train_sgd[k])\n",
        "      solution_fitness = acc_meter.result().numpy() * 100\n",
        "      acc_meter.reset_states()\n",
        "    else:\n",
        "      acc_meter.update_state(tf.argmax(model.predict(X_train), axis=1), y_train)\n",
        "      solution_fitness = acc_meter.result().numpy() * 100\n",
        "      acc_meter.reset_states()\n",
        "    return solution_fitness"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1eTVN72hDnR"
      },
      "source": [
        "# Crossover function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52QeidT1oTdx"
      },
      "source": [
        "def crossover_func(parents, offspring_size, ga_instance):\n",
        "  global cycles_stucked, tmp, parent1\n",
        "  offspring = []\n",
        "  idx = 0\n",
        "\n",
        "  #Standard offspring method from https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#life-cycle-of-pygad\n",
        "  while len(offspring) != offspring_size[0]:\n",
        "    parent1 = parents[idx % parents.shape[0], :].copy()\n",
        "    parent2 = parents[(idx + 1) % parents.shape[0], :].copy()\n",
        "    random_split_point = numpy.random.choice(range(offspring_size[0]))\n",
        "    parent1[random_split_point:] = parent2[random_split_point:]\n",
        "    offspring.append(parent1)\n",
        "    idx += 1\n",
        "\n",
        "  offspring = numpy.array(offspring)\n",
        "  \n",
        "  if introduce_new_pop and cycles_stucked == max_cycles_stucked:\n",
        "    #replace the first num_introduce with random variables\n",
        "    offspring[:num_introduce] = tf.random.truncated_normal(offspring[:num_introduce].shape).numpy()\n",
        "    print('New gene added')\n",
        "    #Only want to do this once\n",
        "    cycles_stucked = 0\n",
        "\n",
        "  return numpy.array(offspring)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMq0ymNksDWa"
      },
      "source": [
        "# callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymw9h30fsBTr"
      },
      "source": [
        "def callback_generation(ga_instance):\n",
        "    global i, k, cycles_stucked, tmp\n",
        "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
        "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
        "    #keep track of generation\n",
        "    i += 1\n",
        "    if sgd_like == True:\n",
        "      if i % (1 * epoch) == 0:\n",
        "        #select random data from reshaped dataset to train\n",
        "        k = numpy.random.randint(0, len(X_train_sgd)-1)\n",
        "\n",
        "    if ga_instance.best_solution()[1] == tmp:\n",
        "      cycles_stucked += 1\n",
        "      if dynamic_batch_size and cycles_stucked == max_cycles_stucked:\n",
        "        #increase batch size. Up until only 1 batch with all samples avaliable\n",
        "        make_batch(min(X_train_sgd.shape[1]+batch_increase, X_train_sgd.shape[0]*X_train_sgd.shape[1]))\n",
        "        print(\"Batch size increased -> new shape: \",X_train_sgd.shape)\n",
        "        #Otherwise introduce_new_pop from cross over will never be True. \n",
        "        if not introduce_new_pop:\n",
        "          #keep track of cycles stucked\n",
        "          cycles_stucked = 0\n",
        "    else:\n",
        "      cycles_stucked = 0\n",
        "    #update tmp\n",
        "    tmp = ga_instance.best_solution()[1]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rfuRuDMuCfg"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ektLvZqDuT8"
      },
      "source": [
        "As observed, rate of accurate increase getting worse and woser. As if a log function. Indeed, it's when GA reached local best. Although over time, GA will get of there (smh given enough time you can brute force AES), but by introducing new genes might* help accerate this process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0e-_WFceeVS",
        "outputId": "01a0114f-6ec2-4aab-b1c7-e666f87e1c4e"
      },
      "source": [
        "keras_ga = pygad.kerasga.KerasGA(model=model,\n",
        "                                 num_solutions=num_solution)\n",
        "\n",
        "\n",
        "# Prepare the PyGAD parameters. Check the documentation for more information: https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#pygad-ga-class\n",
        "# Use the pre-existing model weight as init population. \n",
        "initial_population = numpy.tile(pygad.kerasga.model_weights_as_vector(model), (num_solution, 1))\n",
        "\n",
        "\n",
        "ga_instance = pygad.GA(num_generations=num_generations,\n",
        "                       num_parents_mating=num_parents_mating,\n",
        "                       initial_population=initial_population,\n",
        "                       fitness_func=fitness_func,\n",
        "                       keep_parents=keep_parents,\n",
        "                       mutation_type=mutation_type,\n",
        "                       parent_selection_type=parent_selection_type,\n",
        "                       crossover_type=crossover_func,\n",
        "                       mutation_num_genes=mutation_num_genes,\n",
        "                       on_generation=callback_generation)\n",
        "\n",
        "ga_instance.run()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation = 1\n",
            "Fitness    = 14.807966351509094\n",
            "Generation = 2\n",
            "Fitness    = 17.666666209697723\n",
            "Generation = 3\n",
            "Fitness    = 14.807966351509094\n",
            "Generation = 4\n",
            "Fitness    = 17.666666209697723\n",
            "Generation = 5\n",
            "Fitness    = 17.666666209697723\n",
            "Generation = 6\n",
            "Fitness    = 17.666666209697723\n",
            "Generation = 7\n",
            "Fitness    = 17.666666209697723\n",
            "Generation = 8\n",
            "Fitness    = 17.666666209697723\n",
            "Generation = 9\n",
            "Fitness    = 17.666666209697723\n",
            "Generation = 10\n",
            "Fitness    = 17.666666209697723\n",
            "Generation = 11\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 12\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 13\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 14\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 15\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 16\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 17\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 18\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 19\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 20\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 21\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 22\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 23\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 24\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 25\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 26\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 27\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 28\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 29\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 30\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 31\n",
            "Fitness    = 19.333332777023315\n",
            "New gene added\n",
            "Generation = 32\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 33\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 34\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 35\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 36\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 37\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 38\n",
            "Fitness    = 19.333332777023315\n",
            "Generation = 39\n",
            "Fitness    = 21.666666865348816\n",
            "Generation = 40\n",
            "Fitness    = 21.666666865348816\n",
            "Generation = 41\n",
            "Fitness    = 21.99999988079071\n",
            "Generation = 42\n",
            "Fitness    = 21.99999988079071\n",
            "Generation = 43\n",
            "Fitness    = 21.99999988079071\n",
            "Generation = 44\n",
            "Fitness    = 21.99999988079071\n",
            "Generation = 45\n",
            "Fitness    = 22.333332896232605\n",
            "Generation = 46\n",
            "Fitness    = 22.333332896232605\n",
            "Generation = 47\n",
            "Fitness    = 22.333332896232605\n",
            "Generation = 48\n",
            "Fitness    = 22.333332896232605\n",
            "Generation = 49\n",
            "Fitness    = 22.333332896232605\n",
            "Generation = 50\n",
            "Fitness    = 22.333332896232605\n",
            "Generation = 51\n",
            "Fitness    = 22.333332896232605\n",
            "Generation = 52\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 53\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 54\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 55\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 56\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 57\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 58\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 59\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 60\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 61\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 62\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 63\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 64\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 65\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 66\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 67\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 68\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 69\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 70\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 71\n",
            "Fitness    = 26.333332061767578\n",
            "Generation = 72\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 73\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 74\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 75\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 76\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 77\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 78\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 79\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 80\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 81\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 82\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 83\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 84\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 85\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 86\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 87\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 88\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 89\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 90\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 91\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 92\n",
            "Fitness    = 27.000001072883606\n",
            "New gene added\n",
            "Generation = 93\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 94\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 95\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 96\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 97\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 98\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 99\n",
            "Fitness    = 27.000001072883606\n",
            "Generation = 100\n",
            "Fitness    = 28.333333134651184\n",
            "Generation = 1\n",
            "Fitness    = 11.999999731779099\n",
            "Generation = 2\n",
            "Fitness    = 22.333332896232605\n",
            "Generation = 3\n",
            "Fitness    = 22.333332896232605\n",
            "Generation = 4\n",
            "Fitness    = 22.333332896232605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQYVcXPuuFMx"
      },
      "source": [
        "# Stat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKFLa_wLeeVT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "fb731db0-838b-486d-a5c5-e2bd8ebaf089"
      },
      "source": [
        "\n",
        "# After the generations complete, some plots are showed that summarize how the outputs/fitness values evolve over generations.\n",
        "ga_instance.plot_fitness(title=\"PyGAD & Keras - Iteration vs. Fitness\", linewidth=4)\n",
        "\n",
        "# Returning the details of the best solution.\n",
        "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n",
        "print(\"Index of the best solution : {solution_idx}\".format(solution_idx=solution_idx))\n",
        "\n",
        "# Fetch the parameters of the best solution.\n",
        "best_solution_weights = pygad.kerasga.model_weights_as_matrix(model=model,\n",
        "                                                              weights_vector=solution)\n",
        "model.set_weights(best_solution_weights)\n",
        "\n",
        "acc_meter.update_state(tf.argmax(model.predict(X_test), axis=1), tf.argmax(y_test, axis=1))\n",
        "print(\"Accuracy on testing set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
        "print(\"Error rate on testing set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))\n",
        "\n",
        "acc_meter.update_state(tf.argmax(model.predict(X_train), axis=1), y_train)\n",
        "print(\"Accuracy on training set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
        "print(\"Error rate on training set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitness value of the best solution = 28.333333134651184\n",
            "Index of the best solution : 4\n",
            "Accuracy on testing set:27.329999208450317%\n",
            "Error rate on testing set:72.67000079154968%\n",
            "Accuracy on training set:26.37142837047577%\n",
            "Error rate on training set:73.62857162952423%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmkaN8GtfaSl"
      },
      "source": [
        "The best run I got is 58%, which I got by running the entire training set. It took my 23 hours to finish 500 cycles with i7 10700f and gtx 2060 running in wsl2 on windows 11. It got stuck on 58% for the last 142 cycles with everything set to false. Still playing around with it and we'll see if I can get to to reach 90% without waiting for days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggp61hKAhkBv"
      },
      "source": [
        "I am gonna to try something new. \n",
        "1. Train 5 models on a medium size batch\n",
        "2. Try to generlize those 5 with rotating random small batches\n",
        "3. Cross over those 5 models\n",
        "4. Repeat 2, 3 until reach >90% or failed. <br>\n",
        "we'll see"
      ]
    }
  ]
}