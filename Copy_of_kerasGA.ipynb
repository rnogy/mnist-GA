{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_kerasGA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lqbeWAze76Q",
        "scrolled": true
      },
      "source": [
        "#!pip install pygad"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu_yNfc2kEBP"
      },
      "source": [
        "Modified version of pygad samples. https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#life-cycle-of-pygad <br>\n",
        "Model inspired and modified from http://uhurumkate.blogspot.com/2018/06/tiny-model-for-mnist-dataset.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N1PbHsWQWlW"
      },
      "source": [
        "You're highly encouraged to use a GPU as it's going to spent most resources on predicting and calculating acc. Please also note that pygad library only use CPU paralleal in some cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXxmJd3ai2yd"
      },
      "source": [
        "The highest acc was achieved with everything set to False, and with 23 hours of training. 500 cycles and 58% acc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5L3JMNgS-L0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import losses, datasets, layers, optimizers, Sequential, metrics\n",
        "import pygad.kerasga\n",
        "import numpy\n",
        "import pygad"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdZ1owxPPsRn"
      },
      "source": [
        "# Hyper param"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGFxRvr3eWGX"
      },
      "source": [
        "acc_meter = tf.keras.metrics.Accuracy()\n",
        "\n",
        "#Instead of training a entire dataset, do small batches.\n",
        "sgd_like = True\n",
        "\n",
        "#introduce new population when stucked. Often time it'll stuck in local best solution.\n",
        "introduce_new_pop = True\n",
        "\n",
        "#increase batch size when stcuked.\n",
        "#Error occurs when using google colab. Not sure if it's google issue. \n",
        "#Will try on my desktop once I return next friday.\n",
        "dynamic_batch_size = False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl1q_FZnfTG1"
      },
      "source": [
        "#num cyckes\n",
        "num_generations = 100\n",
        "\n",
        "#num soltions\n",
        "num_solution= 10\n",
        "\n",
        "#the following two param only works if introduce_new_pop = True\n",
        "#Maxium cycles stuck before introducing new population to the pool.\n",
        "max_cycles_stucked = 20\n",
        "#Number of population being replaced with new random population. Must be 0 < x <= num_solution\n",
        "num_introduce = 7\n",
        "\n",
        "#The follow two param have effect only when sgd_like = True\n",
        "#number of cycles you want on a single batch. \n",
        "epoch = 20\n",
        "#number of samples per batch\n",
        "batch_size = 1000\n",
        "\n",
        "#number batch increase feed into prediction. only have effect if sgd_like = True\n",
        "#Ex. batch size 300 -> 300 + 300 = 600. Note that some data will be lose during the process\n",
        "batch_increase = 500\n",
        "\n",
        "#Typical pygad params\n",
        "num_parents_mating = int(num_solution * .7)\n",
        "parent_selection_type = \"sss\"\n",
        "mutation_type=\"adaptive\"\n",
        "mutation_num_genes=(90, 80)\n",
        "keep_parents = 3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm_Hl7BAgrzv"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rkq5w4dequA"
      },
      "source": [
        "#load mnist dataset\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
        "#normalize data\n",
        "X_train = 2*tf.convert_to_tensor(X_train, tf.float32)/255.-1\n",
        "#[b, 28, 28] -> [b, 28, 28, 1]\n",
        "X_train = tf.reshape(X_train, (-1, 28,28,1))\n",
        "X_test = 2*tf.convert_to_tensor(X_test, tf.float32)/255.-1\n",
        "X_test = tf.reshape(X_test, (-1, 28,28,1))\n",
        "y_train = tf.convert_to_tensor(y_train, tf.int32)\n",
        "#[b] -> [b, 10]\n",
        "y_train = tf.one_hot(y_train, 20)\n",
        "y_test = tf.convert_to_tensor(y_test, tf.int32)\n",
        "y_test = tf.one_hot(y_test, 10)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGsLcsClp1P6"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYvgt7ULiR5L"
      },
      "source": [
        "We want a very small model for the increase in param will increase the cycles GA needs to run </ba>\n",
        "Model parameter = 2665 with acc ~ 96% on both train/testing sets (trained with triditional method).<br>\n",
        "EDIT: <br>\n",
        " I relized that the model **DO** have impact. For example, when l`ayers.Dense(3, activation='relu')`, you'll have 1.8k param. You might think it'll be easier for the model to converge. Yet, `layers.Dense(10, activation='relu')` is faster. `layers.Dense(20, activation='relu')` is even better with 4.5k param. This indicates we don't need a small model, but a GOOD model (define good)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW1AXe0zci0T",
        "outputId": "d7bd6664-97c1-48e7-82c7-c34beb14e4e9"
      },
      "source": [
        "model = Sequential([\n",
        "#input layer\n",
        "layers.Convolution2D(15, (3, 3), activation='relu', input_shape=(28,28,1)),\n",
        "layers.BatchNormalization(momentum=0.1),\n",
        "layers.Dropout(0.1),\n",
        "layers.AveragePooling2D(2),\n",
        "#convo layers1\n",
        "layers.Convolution2D(10,(1,1), activation='relu'),\n",
        "layers.BatchNormalization(momentum=0.1),\n",
        "layers.Dropout(0.1),\n",
        "#convo layers2\n",
        "layers.AveragePooling2D(2),\n",
        "layers.Convolution2D(5,(3,3), activation='relu'),\n",
        "layers.BatchNormalization(momentum=0.1),\n",
        "layers.Dropout(0.1),\n",
        "#convo layers3\n",
        "layers.Convolution2D(10,(1,1), activation='relu'),\n",
        "layers.Dropout(0.1),\n",
        "#fully connected layer1\n",
        "layers.Dense(32, activation='relu'),\n",
        "layers.Flatten(),\n",
        "#output layer\n",
        "layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 15)        150       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 26, 26, 15)        60        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 26, 26, 15)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 13, 13, 15)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 10)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 13, 13, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 10)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 5)           455       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 4, 5)           20        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 5)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 10)          60        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 10)          0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4, 4, 32)          352       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 6,427\n",
            "Trainable params: 6,367\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy2Zi5Wdip3p"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "#model.fit(X_train, y_train, \n",
        "#          batch_size=8, verbose=1,shuffle=True,\n",
        "#          validation_data=(X_test, y_test))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV6R1V52ptP7"
      },
      "source": [
        "Network accuracy and error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSVRcjVijtHY",
        "outputId": "daddb019-ffce-4774-cf4d-b5e7f10cec57"
      },
      "source": [
        "acc_meter.update_state(tf.argmax(model.predict(X_test), axis=1), tf.argmax(y_test, axis=1))\n",
        "print(\"Accuracy on testing set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
        "print(\"Error rate on testing set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))\n",
        "acc_meter.update_state(tf.argmax(model.predict(X_train), axis=1), tf.argmax(y_train, axis=1))\n",
        "print(\"Accuracy on training set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
        "print(\"Error rate on training set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set:11.969999969005585%\n",
            "Error rate on testing set:88.03000003099442%\n",
            "Accuracy on training set:11.562857031822205%\n",
            "Error rate on training set:88.4371429681778%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ptOJ9miprDS"
      },
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_omnjFA5lOh"
      },
      "source": [
        "#cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "#cce(y_train, model.predict(X_train))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1Q4nna4YeCz"
      },
      "source": [
        "#we don't need one-hot to train it's for tf.fit(). To compute tf.argmax each loop is a waste of resources\n",
        "y_train = tf.argmax(y_train, axis=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYoIXfZjp4kJ"
      },
      "source": [
        "Perpare batches if we were doing SGD-like method. <br>\n",
        "Some data are removed if size of dataset / batch_size is not an integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMeNPbTi_E04"
      },
      "source": [
        "#A method always modifing X_train_sgd and y_train_sgd\n",
        "def make_batch(batch_size):\n",
        "  global X_train_sgd, y_train_sgd\n",
        "  if len(X_train) % batch_size == 0:\n",
        "    X_train_sgd = tf.reshape(X_train, [-1, batch_size, 28, 28, 1])\n",
        "    y_train_sgd = tf.reshape(y_train, [-1, batch_size])\n",
        "  else:\n",
        "    #Remove some samples if can't rehape \n",
        "    len_remove = len(X_train) % batch_size\n",
        "    X_train_sgd = tf.reshape(X_train[:-len_remove], [-1, batch_size, 28, 28, 1])\n",
        "    y_train_sgd = tf.reshape(y_train[:-len_remove], [-1, batch_size])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V7t28kieeVR"
      },
      "source": [
        "X_train_sgd = []\n",
        "y_train_sgd = []\n",
        "if sgd_like == True:\n",
        "  make_batch(batch_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROmBNKL2Pw2u"
      },
      "source": [
        "# Fitness "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuGqTK0NeeVR"
      },
      "source": [
        "#Some helper varibles\n",
        "i = 0\n",
        "cycles_stucked = 0\n",
        "k=0\n",
        "tmp = 0\n",
        "\n",
        "#fitness_func uses acc matrix as fitness.\n",
        "def fitness_func(solution, sol_idx):\n",
        "\n",
        "    global keras_ga, model, i, cycles_stucked ,X_train, y_train, k\n",
        "    solution_fitness = 0\n",
        "    model_weights_matrix = pygad.kerasga.model_weights_as_matrix(model=model,\n",
        "                                                                 weights_vector=solution)\n",
        "\n",
        "    model.set_weights(weights=model_weights_matrix)\n",
        "\n",
        "    #if perfer to use loss instead\n",
        "    #solution_fitness = 1/cce(model.predict(X_train), y_train).numpy()\n",
        "\n",
        "    if sgd_like == True:\n",
        "      #update acc_meter\n",
        "      acc_meter.update_state(tf.argmax(model.predict(X_train_sgd[k]), axis=1), y_train_sgd[k])\n",
        "      solution_fitness = acc_meter.result().numpy() * 100\n",
        "      acc_meter.reset_states()\n",
        "    else:\n",
        "      acc_meter.update_state(tf.argmax(model.predict(X_train), axis=1), y_train)\n",
        "      solution_fitness = acc_meter.result().numpy() * 100\n",
        "      acc_meter.reset_states()\n",
        "    return solution_fitness"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1eTVN72hDnR"
      },
      "source": [
        "# Crossover function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52QeidT1oTdx"
      },
      "source": [
        "def crossover_func(parents, offspring_size, ga_instance):\n",
        "  global cycles_stucked, tmp, parent1\n",
        "  offspring = []\n",
        "  idx = 0\n",
        "\n",
        "  #Standard offspring method from https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#life-cycle-of-pygad\n",
        "  while len(offspring) != offspring_size[0]:\n",
        "    parent1 = parents[idx % parents.shape[0], :].copy()\n",
        "    parent2 = parents[(idx + 1) % parents.shape[0], :].copy()\n",
        "    random_split_point = numpy.random.choice(range(offspring_size[0]))\n",
        "    parent1[random_split_point:] = parent2[random_split_point:]\n",
        "    offspring.append(parent1)\n",
        "    idx += 1\n",
        "\n",
        "  offspring = numpy.array(offspring)\n",
        "  \n",
        "  if introduce_new_pop and cycles_stucked == max_cycles_stucked:\n",
        "    #replace the first num_introduce with random variables\n",
        "    offspring[:num_introduce] = tf.random.truncated_normal(offspring[:num_introduce].shape).numpy()\n",
        "    print('New gene added')\n",
        "    #Only want to do this once\n",
        "    cycles_stucked = 0\n",
        "\n",
        "  return numpy.array(offspring)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMq0ymNksDWa"
      },
      "source": [
        "# callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymw9h30fsBTr"
      },
      "source": [
        "def callback_generation(ga_instance):\n",
        "    global i, k, cycles_stucked, tmp\n",
        "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
        "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
        "    #keep track of generation\n",
        "    i += 1\n",
        "    if sgd_like == True:\n",
        "      if i % (1 * epoch) == 0:\n",
        "        #select random data from reshaped dataset to train\n",
        "        k = numpy.random.randint(0, len(X_train_sgd)-1)\n",
        "\n",
        "    if ga_instance.best_solution()[1] == tmp:\n",
        "      cycles_stucked += 1\n",
        "      if dynamic_batch_size and cycles_stucked == max_cycles_stucked:\n",
        "        #increase batch size. Up until only 1 batch with all samples avaliable\n",
        "        make_batch(min(X_train_sgd.shape[1]+batch_increase, X_train_sgd.shape[0]*X_train_sgd.shape[1]))\n",
        "        print(\"Batch size increased -> new shape: \",X_train_sgd.shape)\n",
        "        #Otherwise introduce_new_pop from cross over will never be True. \n",
        "        if not introduce_new_pop:\n",
        "          #keep track of cycles stucked\n",
        "          cycles_stucked = 0\n",
        "    else:\n",
        "      cycles_stucked = 0\n",
        "    #update tmp\n",
        "    tmp = ga_instance.best_solution()[1]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rfuRuDMuCfg"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ektLvZqDuT8"
      },
      "source": [
        "As observed, rate of accurate increase getting worse and woser. As if a log function. Indeed, it's when GA reached local best. Although over time, GA will get of there (smh given enough time you can brute force AES), but by introducing new genes might* help accerate this process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0e-_WFceeVS",
        "outputId": "bd51e331-2700-45dd-de4f-f00a903035ee"
      },
      "source": [
        "keras_ga = pygad.kerasga.KerasGA(model=model,\n",
        "                                 num_solutions=num_solution)\n",
        "\n",
        "\n",
        "# Prepare the PyGAD parameters. Check the documentation for more information: https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#pygad-ga-class\n",
        "# Use the pre-existing model weight as init population. \n",
        "initial_population = numpy.tile(pygad.kerasga.model_weights_as_vector(model), (num_solution, 1))\n",
        "\n",
        "\n",
        "ga_instance = pygad.GA(num_generations=num_generations,\n",
        "                       num_parents_mating=num_parents_mating,\n",
        "                       initial_population=initial_population,\n",
        "                       fitness_func=fitness_func,\n",
        "                       keep_parents=keep_parents,\n",
        "                       mutation_type=mutation_type,\n",
        "                       parent_selection_type=parent_selection_type,\n",
        "                       crossover_type=crossover_func,\n",
        "                       mutation_num_genes=mutation_num_genes,\n",
        "                       on_generation=callback_generation)\n",
        "\n",
        "ga_instance.run()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation = 1\n",
            "Fitness    = 11.54366210103035\n",
            "Generation = 2\n",
            "Fitness    = 11.54366210103035\n",
            "Generation = 3\n",
            "Fitness    = 16.200000047683716\n",
            "Generation = 4\n",
            "Fitness    = 13.600000739097595\n",
            "Generation = 5\n",
            "Fitness    = 16.200000047683716\n",
            "Generation = 6\n",
            "Fitness    = 16.200000047683716\n",
            "Generation = 7\n",
            "Fitness    = 16.200000047683716\n",
            "Generation = 8\n",
            "Fitness    = 16.79999977350235\n",
            "Generation = 9\n",
            "Fitness    = 17.399999499320984\n",
            "Generation = 10\n",
            "Fitness    = 17.399999499320984\n",
            "Generation = 11\n",
            "Fitness    = 17.399999499320984\n",
            "Generation = 12\n",
            "Fitness    = 19.300000369548798\n",
            "Generation = 13\n",
            "Fitness    = 19.300000369548798\n",
            "Generation = 14\n",
            "Fitness    = 19.300000369548798\n",
            "Generation = 15\n",
            "Fitness    = 19.300000369548798\n",
            "Generation = 16\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 17\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 18\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 19\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 20\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 21\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 22\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 23\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 24\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 25\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 26\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 27\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 28\n",
            "Fitness    = 22.100000083446503\n",
            "Generation = 29\n",
            "Fitness    = 23.600000143051147\n",
            "Generation = 30\n",
            "Fitness    = 23.600000143051147\n",
            "Generation = 31\n",
            "Fitness    = 24.300000071525574\n",
            "Generation = 32\n",
            "Fitness    = 24.300000071525574\n",
            "Generation = 33\n",
            "Fitness    = 24.300000071525574\n",
            "Generation = 34\n",
            "Fitness    = 24.300000071525574\n",
            "Generation = 35\n",
            "Fitness    = 24.300000071525574\n",
            "Generation = 36\n",
            "Fitness    = 25.40000081062317\n",
            "Generation = 37\n",
            "Fitness    = 25.40000081062317\n",
            "Generation = 38\n",
            "Fitness    = 25.40000081062317\n",
            "Generation = 39\n",
            "Fitness    = 25.40000081062317\n",
            "Generation = 40\n",
            "Fitness    = 25.40000081062317\n",
            "Generation = 41\n",
            "Fitness    = 25.40000081062317\n",
            "Generation = 42\n",
            "Fitness    = 25.40000081062317\n",
            "Generation = 43\n",
            "Fitness    = 25.7999986410141\n",
            "Generation = 44\n",
            "Fitness    = 25.7999986410141\n",
            "Generation = 45\n",
            "Fitness    = 25.7999986410141\n",
            "Generation = 46\n",
            "Fitness    = 25.7999986410141\n",
            "Generation = 47\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 48\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 49\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 50\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 51\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 52\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 53\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 54\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 55\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 56\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 57\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 58\n",
            "Fitness    = 26.199999451637268\n",
            "Generation = 59\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 60\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 61\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 62\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 63\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 64\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 65\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 66\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 67\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 68\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 69\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 70\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 71\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 72\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 73\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 74\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 75\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 76\n",
            "Fitness    = 27.300000190734863\n",
            "Generation = 77\n",
            "Fitness    = 27.700001001358032\n",
            "Generation = 78\n",
            "Fitness    = 31.400001049041748\n",
            "Generation = 79\n",
            "Fitness    = 31.400001049041748\n",
            "Generation = 80\n",
            "Fitness    = 31.400001049041748\n",
            "Generation = 81\n",
            "Fitness    = 31.400001049041748\n",
            "Generation = 82\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 83\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 84\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 85\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 86\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 87\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 88\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 89\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 90\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 91\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 92\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 93\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 94\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 95\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 96\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 97\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 98\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 99\n",
            "Fitness    = 31.799998879432678\n",
            "Generation = 100\n",
            "Fitness    = 31.799998879432678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQYVcXPuuFMx"
      },
      "source": [
        "# Stat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKFLa_wLeeVT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "53dc496f-1854-48c0-ce3e-85608f16b567"
      },
      "source": [
        "\n",
        "# After the generations complete, some plots are showed that summarize how the outputs/fitness values evolve over generations.\n",
        "ga_instance.plot_fitness(title=\"PyGAD & Keras - Iteration vs. Fitness\", linewidth=4)\n",
        "\n",
        "# Returning the details of the best solution.\n",
        "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
        "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n",
        "print(\"Index of the best solution : {solution_idx}\".format(solution_idx=solution_idx))\n",
        "\n",
        "# Fetch the parameters of the best solution.\n",
        "best_solution_weights = pygad.kerasga.model_weights_as_matrix(model=model,\n",
        "                                                              weights_vector=solution)\n",
        "model.set_weights(best_solution_weights)\n",
        "\n",
        "acc_meter.update_state(tf.argmax(model.predict(X_test), axis=1), tf.argmax(y_test, axis=1))\n",
        "print(\"Accuracy on testing set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
        "print(\"Error rate on testing set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))\n",
        "\n",
        "acc_meter.update_state(tf.argmax(model.predict(X_train), axis=1), y_train)\n",
        "print(\"Accuracy on training set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
        "print(\"Error rate on training set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitness value of the best solution = 31.799998879432678\n",
            "Index of the best solution : 0\n",
            "Accuracy on testing set:32.829999923706055%\n",
            "Error rate on testing set:67.17000007629395%\n",
            "Accuracy on training set:32.08142817020416%\n",
            "Error rate on training set:67.91857182979584%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmkaN8GtfaSl"
      },
      "source": [
        "The best run I got is 58%, which I got by running the entire training set. It took my 23 hours to finish 500 cycles with i7 10700f and gtx 2060 running in wsl2 on windows 11. It got stuck on 58% for the last 142 cycles with everything set to false. Still playing around with it and we'll see if I can get to to reach 90% without waiting for days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggp61hKAhkBv"
      },
      "source": [
        "I am gonna to try something new. \n",
        "1. Train 5 models on a medium size batch\n",
        "2. Try to generlize those 5 with rotating random small batches\n",
        "3. Cross over those 5 models\n",
        "4. Repeat 2, 3 until reach >90% or failed. <br>\n",
        "we'll see"
      ]
    }
  ]
}