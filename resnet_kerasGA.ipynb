{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8lqbeWAze76Q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install pygad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu_yNfc2kEBP"
   },
   "source": [
    "Modified version of pygad samples. https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#life-cycle-of-pygad <br>\n",
    "Model inspired and modified from http://uhurumkate.blogspot.com/2018/06/tiny-model-for-mnist-dataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-N1PbHsWQWlW"
   },
   "source": [
    "You're highly encouraged to use a GPU as it's going to spent most resources on predicting and calculating acc. Please also note that pygad library only use CPU paralleal in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXxmJd3ai2yd"
   },
   "source": [
    "The highest acc was achieved with everything set to False, and with 23 hours of training. 500 cycles and 58% acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i5L3JMNgS-L0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import losses, datasets, layers, optimizers, Sequential, metrics\n",
    "import pygad.kerasga\n",
    "import numpy\n",
    "import math\n",
    "import pygad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv9u4kZrkL6h"
   },
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ntboncMNkL6h"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 08:11:55.499500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-10 08:11:55.555641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-10 08:11:55.556399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-10 08:11:55.557629: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-10 08:11:55.560864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-10 08:11:55.561420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-10 08:11:55.561761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-10 08:11:56.403565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-10 08:11:56.404035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-10 08:11:56.404150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1594] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2021-12-10 08:11:56.404422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-10 08:11:56.404561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5980 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = tensorflow.keras.applications.resnet50.ResNet50(\n",
    "    include_top=True, weights=None, input_tensor=None,\n",
    "    input_shape=(32,32,1), pooling=None, classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdZ1owxPPsRn"
   },
   "source": [
    "# Hyper param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JGFxRvr3eWGX"
   },
   "outputs": [],
   "source": [
    "acc_meter = tf.keras.metrics.Accuracy()\n",
    "\n",
    "#Instead of training a entire dataset, do small batches.\n",
    "sgd_like = True\n",
    "\n",
    "#introduce new population when stucked. Often time it'll stuck in local optima.\n",
    "introduce_new_pop = False\n",
    "\n",
    "#increase batch size when stcuked.\n",
    "dynamic_batch_size = True\n",
    "\n",
    "#reshuffle\n",
    "reshuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hl1q_FZnfTG1"
   },
   "outputs": [],
   "source": [
    "#num cycles\n",
    "num_generations = 10000\n",
    "\n",
    "#num soltions\n",
    "num_solution= 7\n",
    "\n",
    "#the following two param only works if introduce_new_pop = True\n",
    "#Maxium cycles stuck before introducing new population to the pool.\n",
    "max_cycles_stucked = 20\n",
    "\n",
    "#The follow two param have effect only when sgd_like = True\n",
    "#number of cycles you want on a single batch. \n",
    "epoch = 5\n",
    "#number of samples per batch\n",
    "batch_size = 64\n",
    "\n",
    "#number batch increase feed into prediction. only have effect if sgd_like = True\n",
    "#Ex. batch size 1500 -> 1500 + 500 = 2000. Note that some data will be lose during the process\n",
    "batch_increase = 1024\n",
    "\n",
    "#reshuffle every n cycles\n",
    "reshuffle_per = (int(60000/batch_size) -1) * epoch #so that it's insync with epoch\n",
    "\n",
    "decay = 1000\n",
    "\n",
    "#Typical pygad params\n",
    "num_parents_mating = int(num_solution * .5) #half parents mating\n",
    "parent_selection_type = \"sss\"\n",
    "mutation_type=\"adaptive\"\n",
    "mutation_lower = 89\n",
    "mutation_num_genes=(90, mutation_lower)\n",
    "keep_parents = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm_Hl7BAgrzv"
   },
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Rkq5w4dequA",
    "outputId": "fef47a88-cbbb-4e81-fde0-65bb028c7fec"
   },
   "outputs": [],
   "source": [
    "#load mnist dataset\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
    "#normalize data\n",
    "X_train = 2*tf.convert_to_tensor(X_train, tf.float32)/255.-1\n",
    "#[b, 28, 28] -> [b, 28, 28, 1]\n",
    "X_train = tf.reshape(X_train, (-1, 28,28,1))\n",
    "X_test = 2*tf.convert_to_tensor(X_test, tf.float32)/255.-1\n",
    "X_test = tf.reshape(X_test, (-1, 28,28,1))\n",
    "y_train = tf.convert_to_tensor(y_train, tf.int32)\n",
    "#[b] -> [b, 10]\n",
    "y_train = tf.one_hot(y_train, 10)\n",
    "y_test = tf.convert_to_tensor(y_test, tf.int32)\n",
    "y_test = tf.one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jTqVaAbnkL6l"
   },
   "outputs": [],
   "source": [
    "X_train = tf.image.resize(X_train, (32,32))\n",
    "X_test = tf.image.resize(X_test, (32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X-gv1SD6kZee"
   },
   "outputs": [],
   "source": [
    "#numpy.reshape(X_train[0], (32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "10jL6g2kkL6l"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "jMhRynyhkL6m",
    "outputId": "f0610fdd-2b6e-439f-b7ed-963340228b4e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(numpy.reshape(X_train[0], (32,32)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGsLcsClp1P6"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYvgt7ULiR5L"
   },
   "source": [
    "We want a very small model for the increase in param will increase the cycles GA needs to run </ba>\n",
    "Model parameter = 2665 with acc ~ 96% on both train/testing sets (trained with triditional method).<br>\n",
    "EDIT: <br>\n",
    " I relized that the model **DO** have impact. For example, when l`ayers.Dense(3, activation='relu')`, you'll have 1.8k param. You might think it'll be easier for the model to converge. Yet, `layers.Dense(10, activation='relu')` is faster. `layers.Dense(20, activation='relu')` is even better with 4.5k param. This indicates we don't need a small model, but a GOOD model (define good)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dW1AXe0zci0T",
    "outputId": "448e58d6-d081-4957-d126-ac6e056b422a"
   },
   "source": [
    "model = Sequential([\n",
    "#input layer\n",
    "layers.Convolution2D(15, (3, 3), activation='relu', input_shape=(28,28,1)),\n",
    "layers.BatchNormalization(momentum=0.1),\n",
    "layers.Dropout(0.1),\n",
    "layers.AveragePooling2D(2),\n",
    "#convo layers1\n",
    "layers.Convolution2D(10,(1,1), activation='relu'),\n",
    "layers.BatchNormalization(momentum=0.1),\n",
    "layers.Dropout(0.1),\n",
    "#convo layers2\n",
    "layers.AveragePooling2D(2),\n",
    "layers.Convolution2D(5,(3,3), activation='relu'),\n",
    "layers.BatchNormalization(momentum=0.1),\n",
    "layers.Dropout(0.1),\n",
    "#convo layers3\n",
    "layers.Convolution2D(10,(1,1), activation='relu'),\n",
    "layers.Dropout(0.1),\n",
    "#fully connected layer1\n",
    "layers.Dense(64, activation='relu'),\n",
    "layers.Dense(32, activation='relu'),\n",
    "layers.Flatten(),\n",
    "#output layer\n",
    "layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg0GLDdGNzSf"
   },
   "source": [
    "maybe FCNN do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SWdcrj3_tvv",
    "outputId": "3d2c057d-ee0c-4d18-aae6-993f7fb48463"
   },
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(28,28,1)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzCE0roAkL6p"
   },
   "source": [
    "def weighted_f1_loss(y_true, y_pred, num_class=10):\n",
    "    #create confusion matrix\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    cm = tf.math.confusion_matrix(y_pred,y_true, num_class)\n",
    "    #sum true positive\n",
    "    tp = tf.math.reduce_sum(cm * tf.eye(num_class, dtype='int32'), 0)\n",
    "    #recall score\n",
    "    rc = tp/tf.math.reduce_sum(cm, 0)\n",
    "    #precision score\n",
    "    pr = tf.math.reduce_sum(cm * tf.eye(num_class, dtype='int32'), 1)\n",
    "    pr = pr / tf.math.reduce_sum(cm, 1)\n",
    "    #f1\n",
    "    f1 = 2 * (pr * rc)/(pr + rc)\n",
    "    #weighted f1\n",
    "    wf1 = tf.cast(tf.math.reduce_sum(cm, 0), 'float64') * f1\n",
    "    wf1 = tf.math.reduce_sum(tf.boolean_mask(wf1, tf.math.is_finite(wf1)))/len(y_true)\n",
    "    #GA works best when given direction -> ** 2\n",
    "    return wf1 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yx5Be7BkL6p"
   },
   "source": [
    "def some_loss(y_true, y_pred):\n",
    "  return tf.reduce_sum(y_true*(tf.math.exp(tf.math.abs(y_pred -y_true))-1)) /len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBJehsNSkL6q"
   },
   "source": [
    "def some_loss(y_true, y_pred):\n",
    "  pt1 = tf.where(y_true == 0, 1E-6, y_true)\n",
    "  return tf.reduce_sum(tf.math.abs(y_pred - pt1)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EIxCtK1-kL6q"
   },
   "outputs": [],
   "source": [
    "def some_loss(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.math.exp(y_pred - y_true)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJX0S-JNkL6q",
    "outputId": "d059a676-676d-4e35-d65b-e899b21f09d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   3200        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 10)           20490       avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,601,930\n",
      "Trainable params: 23,548,810\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4NiU0RFOkL6r"
   },
   "outputs": [],
   "source": [
    "def some_loss(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.math.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNFCsftXkL6r",
    "outputId": "c2ee218f-0e19-46d3-c45a-694e565abc7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 08:11:58.676141: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2021-12-10 08:11:59.729099: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing set:11.59999966621399%\n",
      "Error rate on testing set:88.40000033378601%\n",
      "Accuracy on training set:10.80000028014183%\n",
      "Error rate on training set:89.19999971985817%\n"
     ]
    }
   ],
   "source": [
    "acc_meter.update_state(tf.argmax(model(X_test[:1000], training=False), axis=1), tf.argmax(y_test[:1000], axis=1))\n",
    "print(\"Accuracy on testing set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
    "print(\"Error rate on testing set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))\n",
    "acc_meter.update_state(tf.argmax(model(X_train[:1000], training=False), axis=1), tf.argmax(y_train[:1000], axis=1))\n",
    "print(\"Accuracy on training set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
    "print(\"Error rate on training set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ptOJ9miprDS"
   },
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "c1Q4nna4YeCz"
   },
   "outputs": [],
   "source": [
    "#we don't need one-hot to train it's for tf.fit(). To compute tf.argmax each loop is a waste of resources\n",
    "#y_train = tf.argmax(y_train, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYoIXfZjp4kJ"
   },
   "source": [
    "Perpare batches if we were doing SGD-like method. <br>\n",
    "Some data are removed if size of dataset / batch_size is not an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lMeNPbTi_E04"
   },
   "outputs": [],
   "source": [
    "#A method always modifing X_train_sgd and y_train_sgd\n",
    "def make_batch(batch_size):\n",
    "  idx = numpy.random.permutation(len(X_train))\n",
    "  global X_train_sgd, y_train_sgd\n",
    "  if len(X_train) % batch_size == 0:\n",
    "    X_train_sgd = tf.reshape(X_train.numpy()[idx], [-1, batch_size, 32, 32, 1])\n",
    "    y_train_sgd = tf.reshape(y_train.numpy()[idx], [-1, batch_size, 10])\n",
    "  else:\n",
    "    #Remove some samples if can't rehape \n",
    "    len_remove = len(X_train) % batch_size\n",
    "    X_train_sgd = tf.reshape(X_train.numpy()[idx][:-len_remove], [-1, batch_size, 32, 32, 1])\n",
    "    y_train_sgd = tf.reshape(y_train.numpy()[idx][:-len_remove], [-1, batch_size, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pifJ-HQ-kL6u"
   },
   "outputs": [],
   "source": [
    "def lsm(arr):\n",
    "    x = numpy.arange(len(arr))\n",
    "    n = len(arr)\n",
    "    pt1 = (n * numpy.sum(arr * x)) - (numpy.sum(x)*numpy.sum(arr))\n",
    "    pt2 = n * numpy.sum(x ** 2) - (numpy.sum(x)**2)\n",
    "    m = pt1 / pt2\n",
    "    b = (numpy.sum(arr) - (m*numpy.sum(x))) / n\n",
    "    print(m)\n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fgJVFs1nkL6u"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_V7t28kieeVR"
   },
   "outputs": [],
   "source": [
    "X_train_sgd = []\n",
    "y_train_sgd = []\n",
    "make_batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROmBNKL2Pw2u"
   },
   "source": [
    "# Fitness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAwfxH0VkL6w"
   },
   "outputs": [],
   "source": [
    "keras_ga = pygad.kerasga.KerasGA(model=model,\n",
    "                                 num_solutions=num_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuGqTK0NeeVR"
   },
   "outputs": [],
   "source": [
    "#Some helper varibles\n",
    "i = 0\n",
    "cycles_stucked = 0\n",
    "k=0\n",
    "tmp = 0\n",
    "\n",
    "#fitness_func uses acc matrix as fitness.\n",
    "def fitness_func(solution, sol_idx):\n",
    "\n",
    "    global model, i, cycles_stucked ,X_train, y_train, k, y_train_sgd, X_train_sgd\n",
    "    solution_fitness = 0\n",
    "    model_weights_matrix = pygad.kerasga.model_weights_as_matrix(model=model,\n",
    "                                                                 weights_vector=solution)\n",
    "\n",
    "    model.set_weights(weights=model_weights_matrix)\n",
    "\n",
    "    #if perfer to use loss instead\n",
    "    #solution_fitness = 1/cce(model.predict(X_train), y_train).numpy()\n",
    "\n",
    "    if sgd_like == True:\n",
    "      #update acc_meter\n",
    "      #acc_meter.update_state(tf.argmax(model.predict(X_train_sgd[k]), axis=1), y_train_sgd[k])\n",
    "      #solution_fitness = acc_meter.result().numpy() * 100\n",
    "      #acc_meter.reset_states()\n",
    "      solution_fitness = some_loss(model(X_train_sgd[k], training=False), y_train_sgd[k]).numpy()\n",
    "    else:\n",
    "      #acc_meter.update_state(tf.argmax(model.predict(X_train), axis=1), y_train)\n",
    "      #solution_fitness = acc_meter.result().numpy() * 100\n",
    "      #acc_meter.reset_states()\n",
    "      solution_fitness = some_loss(model(X_train, training=False), y_train).numpy()\n",
    "    #print(solution_fitness)\n",
    "    if numpy.isnan(solution_fitness):\n",
    "      return 1E-20\n",
    "    else:\n",
    "      return 1/ solution_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1eTVN72hDnR"
   },
   "source": [
    "# Crossover function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52QeidT1oTdx"
   },
   "outputs": [],
   "source": [
    "def crossover_func(parents, offspring_size, ga_instance):\n",
    "  global cycles_stucked\n",
    "  offspring = []\n",
    "  idx = 0\n",
    "\n",
    "  #Standard offspring method from https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#life-cycle-of-pygad\n",
    "  while len(offspring) != offspring_size[0]:\n",
    "    parent1 = parents[idx % parents.shape[0], :].copy()\n",
    "    parent2 = parents[(idx + 1) % parents.shape[0], :].copy()\n",
    "    random_split_point = numpy.random.choice(range(offspring_size[0]))\n",
    "    parent1[random_split_point:] = parent2[random_split_point:]\n",
    "    offspring.append(parent1)\n",
    "    idx += 1\n",
    "\n",
    "  offspring = numpy.array(offspring)\n",
    "  \n",
    "  if introduce_new_pop and cycles_stucked == max_cycles_stucked:\n",
    "    #craete new parents with new random variables\n",
    "    new_parents = tf.random.truncated_normal(offspring.shape).numpy()\n",
    "    #create an array that contain both offspring and new parents\n",
    "    new_parents = numpy.append(new_parents, offspring)\n",
    "    #shuffle\n",
    "    numpy.random.shuffle(new_parents)\n",
    "    #rehspae\n",
    "    new_parents = new_parents.reshape(-1, offspring.shape[1])\n",
    "    offspring = new_parents[:len(offspring)]\n",
    "    print('New gene added')\n",
    "    #Only want to do this once\n",
    "    cycles_stucked = 0\n",
    "\n",
    "  return numpy.array(offspring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMq0ymNksDWa"
   },
   "source": [
    "# callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ymw9h30fsBTr"
   },
   "outputs": [],
   "source": [
    "def callback_generation(ga_instance):\n",
    "    global i, k, cycles_stucked, tmp, X_train_sgd, mutation_lower, decay, epoch\n",
    "    \n",
    "    fitness = ga_instance.best_solution()[1]\n",
    "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
    "    print(\"Fitness    = {fitness}\".format(fitness=fitness))\n",
    "\n",
    "    if fitness == tmp:\n",
    "      cycles_stucked += 1\n",
    "      if dynamic_batch_size and cycles_stucked == max_cycles_stucked:\n",
    "        #increase batch size. Up until only 1 batch with all samples avaliable\n",
    "        make_batch(min(X_train_sgd.shape[1]+batch_increase, X_train_sgd.shape[0]*X_train_sgd.shape[1]))\n",
    "        print(\"Batch size increased -> new shape: \",X_train_sgd.shape)\n",
    "        k = numpy.random.randint(0, len(X_train_sgd)-1)\n",
    "        #Otherwise introduce_new_pop from cross over will never be True. \n",
    "        if not introduce_new_pop:\n",
    "          #keep track of cycles stucked\n",
    "          cycles_stucked = 0\n",
    "    else:\n",
    "      cycles_stucked = 0\n",
    "\n",
    "    #keep track of generation\n",
    "    i += 1\n",
    "    if sgd_like == True:\n",
    "      if i % (1 * epoch) == 0:\n",
    "        #select random data from reshaped dataset to train\n",
    "        k = numpy.random.randint(0, len(X_train_sgd)-1)\n",
    "        print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
    "        print('new set')\n",
    "    if reshuffle:\n",
    "      if i % (1 * reshuffle_per) == 0:\n",
    "        #select random data from reshaped dataset to train\n",
    "        make_batch(X_train_sgd.shape[1])\n",
    "        print('reshuffled')\n",
    "    #update tmp\n",
    "    tmp = fitness\n",
    "    \n",
    "    if i % (decay) == 0:\n",
    "        print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
    "        print(\"Fitness    = {fitness}\".format(fitness=fitness))\n",
    "        mutation_lower -= 15\n",
    "        #epoch += 10 \n",
    "        setattr(ga_instance, 'mutation_num_genes', (90, max(5, mutation_lower)))\n",
    "        #make_batch(max(X_train_sgd.shape[1]-batch_increase, 500))\n",
    "        #k = numpy.random.randint(0, len(X_train_sgd)-1)\n",
    "        #decay = max(decay - 50, 100)\n",
    "        print(\"Batch size increased -> new shape: \",X_train_sgd.shape)\n",
    "        print(\"new mutation rate:\", max(30, mutation_lower), \" decay: \", decay)\n",
    "#    if fitness >= 50:\n",
    "#      sgd_like = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rfuRuDMuCfg"
   },
   "source": [
    "#  Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "u0e-_WFceeVS",
    "outputId": "e6b5d481-b1f8-4c20-ff18-79d1b2a296e3"
   },
   "outputs": [],
   "source": [
    "# Prepare the PyGAD parameters. Check the documentation for more information: https://pygad.readthedocs.io/en/latest/README_pygad_ReadTheDocs.html#pygad-ga-class\n",
    "# Use the pre-existing model weight as init population. \n",
    "initial_population = numpy.tile(pygad.kerasga.model_weights_as_vector(model), (num_solution, 1))\n",
    "\n",
    "\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating,\n",
    "                       initial_population=initial_population,\n",
    "                       fitness_func=fitness_func,\n",
    "                       keep_parents=keep_parents,\n",
    "                       mutation_type=mutation_type,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       crossover_type=crossover_func,\n",
    "                       mutation_num_genes=mutation_num_genes,\n",
    "                       on_generation=callback_generation)\n",
    "                       #stop_criteria='reach_85')\n",
    "\n",
    "ga_instance.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQYVcXPuuFMx"
   },
   "source": [
    "# Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "gKFLa_wLeeVT",
    "outputId": "9fdc01bb-d9d7-4fd5-85ad-b1f96844ab7a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# After the generations complete, some plots are showed that summarize how the outputs/fitness values evolve over generations.\n",
    "ga_instance.plot_fitness(title=\"PyGAD & Keras - Iteration vs. Fitness\", linewidth=4)\n",
    "\n",
    "# Returning the details of the best solution.\n",
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n",
    "print(\"Index of the best solution : {solution_idx}\".format(solution_idx=solution_idx))\n",
    "\n",
    "# Fetch the parameters of the best solution.\n",
    "best_solution_weights = pygad.kerasga.model_weights_as_matrix(model=model,\n",
    "                                                              weights_vector=solution)\n",
    "model.set_weights(best_solution_weights)\n",
    "\n",
    "#remove if using acc matrix as fitness instead])\n",
    "#acc_meter.update_state(tf.argmax(model.predict(X_test), axis=1), tf.argmax(y_test, axis=1))\n",
    "#print(\"Accuracy on testing set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
    "#print(\"Error rate on testing set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))\n",
    "\n",
    "#acc_meter.update_state(tf.argmax(model.predict(X_train), axis=1), y_train)\n",
    "#print(\"Accuracy on training set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
    "#print(\"Error rate on training set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5jb0MTrkL60",
    "outputId": "a061f762-b0c5-4d6a-d747-539066511d64"
   },
   "outputs": [],
   "source": [
    "acc_meter.update_state(tf.argmax(model(X_test[:1000], training=False), axis=1), tf.argmax(y_test[:1000], axis=1))\n",
    "print(\"Accuracy on testing set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
    "print(\"Error rate on testing set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))\n",
    "acc_meter.update_state(tf.argmax(model(X_train[:1000], training=False), axis=1), tf.argmax(y_train[:1000], axis=1))\n",
    "print(\"Accuracy on training set:{acc}%\".format(acc=acc_meter.result().numpy()*100))\n",
    "print(\"Error rate on training set:{err}%\".format(err=(1-acc_meter.result().numpy())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Jber5aSkL60"
   },
   "outputs": [],
   "source": [
    "arr = getattr(ga_instance, 'best_solutions_fitness')\n",
    "x = numpy.arange(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "3G7H7zCakL60",
    "outputId": "65f0608d-ae35-429c-bc78-cb89e0fbe89d"
   },
   "outputs": [],
   "source": [
    "plt.plot(x, arr)\n",
    "per = 500\n",
    "i = 0\n",
    "j = i + per\n",
    "while j <= len(arr):\n",
    "    plt.plot(x[i:j], lsm(arr[i:j]), 'yellow')\n",
    "    i = j\n",
    "    j += per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZmRadtdkL60"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "resnet_kerasGA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
